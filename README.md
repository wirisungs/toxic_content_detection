# Toxic Content Detection using Large Language Models

Dự án này tập trung vào việc phát hiện nội dung độc hại sử dụng các mô hình ngôn ngữ lớn (LLM).

## Cấu trúc dự án

```
toxic_content_detection/
├── src/                    # Mã nguồn chính
│   ├── data/              # Xử lý dữ liệu
│   ├── models/            # Mô hình và logic huấn luyện
│   ├── utils/             # Tiện ích và hàm hỗ trợ
│   ├── config/            # Cấu hình
│   ├── api/               # API endpoints
│   └── tests/             # Unit tests
├── data/                  # Dữ liệu
│   ├── raw/              # Dữ liệu thô
│   └── processed/        # Dữ liệu đã xử lý
├── notebooks/            # Jupyter notebooks
└── docs/                 # Tài liệu
```

## Cài đặt

1. Clone repository:
```bash
git clone [repository-url]
cd toxic_content_detection
```

2. Tạo môi trường ảo và cài đặt dependencies:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoặc
.\venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

## Sử dụng

[Thêm hướng dẫn sử dụng ở đây]

## Đóng góp

[Thêm hướng dẫn đóng góp ở đây]

## Giấy phép

[Thêm thông tin giấy phép ở đây]
